{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b7a118d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATASET: kernel-2190e5de-075c-47a8-a2af-667b321e4597.json\n",
      "================================================================================\n",
      "Preview (first 5 rows):\n",
      "  {\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "NaN\n",
      "\n",
      "================================================================================\n",
      "1) JUMLAH DATA\n",
      "================================================================================\n",
      "Rows: 12\n",
      "Columns: 1\n",
      "\n",
      "================================================================================\n",
      "2) KARAKTERISTIK DATA\n",
      "================================================================================\n",
      "Numeric cols: 1 -> ['{']\n",
      "Categorical/other cols: 0 -> []\n",
      "\n",
      "Dtypes:\n",
      "  column    dtype\n",
      "0      {  float64\n",
      "\n",
      "Unique counts per column (top 30):\n",
      "  column  nunique\n",
      "0      {        0\n",
      "\n",
      "================================================================================\n",
      "3) MISSING VALUES\n",
      "================================================================================\n",
      "Columns with missing values (top 50):\n",
      "   missing_count  missing_pct\n",
      "{             12        100.0\n",
      "\n",
      "================================================================================\n",
      "4) DUPLICATE ROWS\n",
      "================================================================================\n",
      "Duplicate rows: 11\n",
      "\n",
      "================================================================================\n",
      "5) RINGKASAN STATISTIK\n",
      "================================================================================\n",
      "Numeric describe (top 30 rows):\n",
      "   count  mean  std  min  25%  50%  75%  max\n",
      "{    0.0   NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "\n",
      "No categorical/object columns detected.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "01_data_checking.py\n",
    "Initial data checking / assessment for CSV datasets:\n",
    "- rows/cols\n",
    "- column types\n",
    "- missing values (count + %)\n",
    "- duplicates\n",
    "- basic descriptive stats (numeric + categorical)\n",
    "- optional: save report files to ./outputs/\n",
    "\n",
    "Usage examples:\n",
    "  python 01_data_checking.py student_combined_data.csv\n",
    "  python 01_data_checking.py student_performance_data.csv student_aptitude_data.csv\n",
    "  python 01_data_checking.py --save student_combined_data.csv\n",
    "  python 01_data_checking.py --save --outdir reports student_combined_data.csv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def safe_read_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read CSV with a couple of safe fallbacks.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        # common fallback\n",
    "        return pd.read_csv(path, encoding=\"latin-1\")\n",
    "\n",
    "\n",
    "def summarize_df(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    n_rows, n_cols = df.shape\n",
    "\n",
    "    # missing\n",
    "    missing_count = df.isna().sum()\n",
    "    missing_pct = (missing_count / max(n_rows, 1) * 100).round(2)\n",
    "\n",
    "    missing_table = (\n",
    "        pd.DataFrame({\"missing_count\": missing_count, \"missing_pct\": missing_pct})\n",
    "        .sort_values([\"missing_count\", \"missing_pct\"], ascending=False)\n",
    "    )\n",
    "\n",
    "    # duplicates\n",
    "    dup_rows = int(df.duplicated().sum())\n",
    "\n",
    "    # dtypes\n",
    "    dtype_table = pd.DataFrame({\"dtype\": df.dtypes.astype(str)}).reset_index().rename(columns={\"index\": \"column\"})\n",
    "\n",
    "    # stats\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = [c for c in df.columns if c not in numeric_cols]\n",
    "\n",
    "    numeric_desc = df[numeric_cols].describe().T if numeric_cols else pd.DataFrame()\n",
    "    categorical_desc = df[cat_cols].describe().T if cat_cols else pd.DataFrame()\n",
    "\n",
    "    # unique counts (useful for \"karakteristik data\")\n",
    "    nunique = df.nunique(dropna=True).sort_values(ascending=False)\n",
    "    nunique_table = pd.DataFrame({\"nunique\": nunique}).reset_index().rename(columns={\"index\": \"column\"})\n",
    "\n",
    "    return {\n",
    "        \"shape\": (n_rows, n_cols),\n",
    "        \"dup_rows\": dup_rows,\n",
    "        \"missing_table\": missing_table,\n",
    "        \"dtype_table\": dtype_table,\n",
    "        \"nunique_table\": nunique_table,\n",
    "        \"numeric_desc\": numeric_desc,\n",
    "        \"categorical_desc\": categorical_desc,\n",
    "        \"numeric_cols\": numeric_cols,\n",
    "        \"cat_cols\": cat_cols,\n",
    "    }\n",
    "\n",
    "\n",
    "def print_header(title: str) -> None:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(title)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def print_compact_table(df: pd.DataFrame, max_rows: int = 30) -> None:\n",
    "    if df.empty:\n",
    "        print(\"(empty)\")\n",
    "        return\n",
    "    if len(df) > max_rows:\n",
    "        print(df.head(max_rows).to_string())\n",
    "        print(f\"... ({len(df) - max_rows} rows omitted)\")\n",
    "    else:\n",
    "        print(df.to_string())\n",
    "\n",
    "\n",
    "def ensure_outdir(outdir: Path) -> None:\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_reports(summary: Dict[str, Any], dataset_name: str, outdir: Path) -> None:\n",
    "    \"\"\"\n",
    "    Save summary tables to csv and a text report.\n",
    "    \"\"\"\n",
    "    ensure_outdir(outdir)\n",
    "\n",
    "    # Save tables\n",
    "    summary[\"missing_table\"].to_csv(outdir / f\"{dataset_name}__missing.csv\")\n",
    "    summary[\"dtype_table\"].to_csv(outdir / f\"{dataset_name}__dtypes.csv\", index=False)\n",
    "    summary[\"nunique_table\"].to_csv(outdir / f\"{dataset_name}__nunique.csv\", index=False)\n",
    "\n",
    "    if not summary[\"numeric_desc\"].empty:\n",
    "        summary[\"numeric_desc\"].to_csv(outdir / f\"{dataset_name}__numeric_desc.csv\")\n",
    "    if not summary[\"categorical_desc\"].empty:\n",
    "        summary[\"categorical_desc\"].to_csv(outdir / f\"{dataset_name}__categorical_desc.csv\")\n",
    "\n",
    "    # Save a human-readable txt report\n",
    "    report_path = outdir / f\"{dataset_name}__report.txt\"\n",
    "    with report_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        n_rows, n_cols = summary[\"shape\"]\n",
    "        f.write(f\"Dataset: {dataset_name}\\n\")\n",
    "        f.write(f\"Shape: {n_rows} rows x {n_cols} cols\\n\")\n",
    "        f.write(f\"Duplicate rows: {summary['dup_rows']}\\n\\n\")\n",
    "\n",
    "        f.write(\"=== Dtypes ===\\n\")\n",
    "        f.write(summary[\"dtype_table\"].to_string(index=False))\n",
    "        f.write(\"\\n\\n=== Unique counts (top) ===\\n\")\n",
    "        f.write(summary[\"nunique_table\"].head(50).to_string(index=False))\n",
    "        f.write(\"\\n\\n=== Missing values (sorted) ===\\n\")\n",
    "        f.write(summary[\"missing_table\"].head(100).to_string())\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        if not summary[\"numeric_desc\"].empty:\n",
    "            f.write(\"\\n=== Numeric describe ===\\n\")\n",
    "            f.write(summary[\"numeric_desc\"].to_string())\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        if not summary[\"categorical_desc\"].empty:\n",
    "            f.write(\"\\n=== Categorical describe ===\\n\")\n",
    "            f.write(summary[\"categorical_desc\"].to_string())\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def main(paths: List[str], save: bool, outdir: str) -> None:\n",
    "    outdir_path = Path(outdir)\n",
    "\n",
    "    for p in paths:\n",
    "        path = Path(p)\n",
    "        if not path.exists():\n",
    "            print_header(f\"[SKIP] File not found: {p}\")\n",
    "            continue\n",
    "\n",
    "        dataset_name = path.stem\n",
    "\n",
    "        print_header(f\"DATASET: {path.name}\")\n",
    "        df = safe_read_csv(path)\n",
    "\n",
    "        # basic preview\n",
    "        print(\"Preview (first 5 rows):\")\n",
    "        print(df.head(5).to_string(index=False))\n",
    "\n",
    "        summary = summarize_df(df)\n",
    "\n",
    "        # 1) jumlah\n",
    "        n_rows, n_cols = summary[\"shape\"]\n",
    "        print_header(\"1) JUMLAH DATA\")\n",
    "        print(f\"Rows: {n_rows}\")\n",
    "        print(f\"Columns: {n_cols}\")\n",
    "\n",
    "        # 2) karakteristik\n",
    "        print_header(\"2) KARAKTERISTIK DATA\")\n",
    "        print(f\"Numeric cols: {len(summary['numeric_cols'])} -> {summary['numeric_cols'][:15]}\" + (\" ...\" if len(summary[\"numeric_cols\"]) > 15 else \"\"))\n",
    "        print(f\"Categorical/other cols: {len(summary['cat_cols'])} -> {summary['cat_cols'][:15]}\" + (\" ...\" if len(summary[\"cat_cols\"]) > 15 else \"\"))\n",
    "\n",
    "        print(\"\\nDtypes:\")\n",
    "        print_compact_table(summary[\"dtype_table\"], max_rows=80)\n",
    "\n",
    "        print(\"\\nUnique counts per column (top 30):\")\n",
    "        print_compact_table(summary[\"nunique_table\"].head(30), max_rows=30)\n",
    "\n",
    "        # 3) missing values\n",
    "        print_header(\"3) MISSING VALUES\")\n",
    "        missing_nonzero = summary[\"missing_table\"][summary[\"missing_table\"][\"missing_count\"] > 0]\n",
    "        if missing_nonzero.empty:\n",
    "            print(\"No missing values found âœ…\")\n",
    "        else:\n",
    "            print(\"Columns with missing values (top 50):\")\n",
    "            print_compact_table(missing_nonzero.head(50), max_rows=50)\n",
    "\n",
    "        # 4) duplicates\n",
    "        print_header(\"4) DUPLICATE ROWS\")\n",
    "        print(f\"Duplicate rows: {summary['dup_rows']}\")\n",
    "\n",
    "        # 5) basic stats\n",
    "        print_header(\"5) RINGKASAN STATISTIK\")\n",
    "        if summary[\"numeric_desc\"].empty:\n",
    "            print(\"No numeric columns detected.\")\n",
    "        else:\n",
    "            print(\"Numeric describe (top 30 rows):\")\n",
    "            print_compact_table(summary[\"numeric_desc\"].head(30), max_rows=30)\n",
    "\n",
    "        if summary[\"categorical_desc\"].empty:\n",
    "            print(\"\\nNo categorical/object columns detected.\")\n",
    "        else:\n",
    "            print(\"\\nCategorical describe (top 30 rows):\")\n",
    "            print_compact_table(summary[\"categorical_desc\"].head(30), max_rows=30)\n",
    "\n",
    "        # save outputs\n",
    "        if save:\n",
    "            print_header(\"SAVING REPORTS\")\n",
    "            save_reports(summary, dataset_name, outdir_path)\n",
    "            print(f\"Saved to: {outdir_path.resolve()}\")\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Initial data checking / assessment for CSV files.\")\n",
    "    parser.add_argument(\n",
    "        \"csv_paths\",\n",
    "        nargs=\"*\",\n",
    "        help=\"Paths to CSV files. If empty, will try common defaults in current folder.\",\n",
    "    )\n",
    "    parser.add_argument(\"--save\", action=\"store_true\", help=\"Save reports to outdir.\")\n",
    "    parser.add_argument(\"--outdir\", default=\"outputs\", help=\"Output directory for saved reports (default: outputs).\")\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    default_candidates = [\n",
    "        \"student_combined_data.csv\",\n",
    "        \"student_performance_data.csv\",\n",
    "        \"student_aptitude_data.csv\",\n",
    "    ]\n",
    "\n",
    "    paths = args.csv_paths if args.csv_paths else default_candidates\n",
    "    main(paths=paths, save=args.save, outdir=args.outdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c47b5-6321-454f-b608-738535970039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
